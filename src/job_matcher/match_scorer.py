"""
MatchScorer - Pass 1: Initial Job Matching

Analyzes jobs against resume and requirements to generate match scores.
Supports multi-threaded processing for faster batch scoring.
"""

import json
import os
import threading
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, Optional, List
from .llama_client import LlamaClient
from .resume_analyzer import ResumeAnalyzer
from .failure_tracker import FailureTracker, ErrorType
from .filters import apply_filters_to_jobs
from .comparison_engine import ComparisonEngine
from .models.job_sections import extract_job_sections
from .smooth_batch_processor import SmoothBatchProcessor


class MatchScorer:
    """Score jobs based on resume and requirements match"""

    def __init__(
        self, llama_client: LlamaClient, resume_analyzer: ResumeAnalyzer,
        checkpoint_manager=None, failure_tracker: Optional[FailureTracker] = None
    ):
        """
        Initialize MatchScorer

        Args:
            llama_client: LlamaClient instance for AI generation
            resume_analyzer: ResumeAnalyzer instance with loaded resume/requirements
            checkpoint_manager: Optional CheckpointManager for resume support
            failure_tracker: Optional FailureTracker for tracking failed jobs
        """
        self.client = llama_client
        self.analyzer = resume_analyzer
        self.checkpoint_manager = checkpoint_manager
        self.failure_tracker = failure_tracker
        self.failed_jobs: List[Dict[str, Any]] = []  # Track failures during batch
        self.rejected_jobs: List[Dict[str, Any]] = []  # Track title-rejected jobs
        self.filtered_jobs: List[Dict[str, Any]] = []  # Track pre-filtered jobs

        # Initialize comparison engine for hybrid scoring
        self.comparison_engine = ComparisonEngine(
            self.analyzer.candidate_profile,
            self.analyzer.preferences
        )

    def _extract_title_keywords(self) -> List[str]:
        """
        Extract target role keywords from requirements

        Returns:
            List of keywords (lowercased) for title matching
        """
        keywords = []

        # Get target roles from candidate profile
        target_roles = self.analyzer.candidate_profile.get("target_roles", [])
        related_keywords = self.analyzer.candidate_profile.get("related_keywords", [])

        # Extract keywords from target roles (individual words)
        for role in target_roles:
            # Split role into words and add them
            words = role.lower().split()
            keywords.extend(words)

        # Add related keywords
        for keyword in related_keywords:
            keywords.append(keyword.lower())

        # Remove common words that don't add value
        stop_words = {"and", "or", "the", "a", "an", "for", "to", "of", "in", "with"}
        keywords = [k for k in keywords if k not in stop_words]

        # Remove duplicates
        return list(set(keywords))

    def _is_title_relevant(self, job: Dict[str, Any]) -> bool:
        """
        Check if job title is relevant based on target keywords

        Args:
            job: Job dict with title field

        Returns:
            True if title contains any target keywords, False otherwise
        """
        job_title = job.get("title", "").lower()
        keywords = self._extract_title_keywords()

        # If no keywords defined, accept all jobs (fallback)
        if not keywords:
            return True

        # Check if job title contains ANY of the keywords
        for keyword in keywords:
            if keyword in job_title:
                return True

        return False

    def score_job(self, job: Dict[str, Any], use_hybrid_scoring: bool = True) -> Optional[Dict[str, Any]]:
        """
        Score a single job against the resume and requirements

        Args:
            job: Job dict from JobPost
            use_hybrid_scoring: If True, combines deterministic + AI scores (default: True)

        Returns:
            Dict with match_score, reasoning, and preference_checks
            Returns None if scoring fails
        """
        try:
            # Calculate deterministic score first
            try:
                deterministic_scores = self.comparison_engine.calculate_deterministic_score(job)
            except Exception as det_error:
                error_msg = f"Deterministic scoring failed: {type(det_error).__name__}: {str(det_error)}"
                print(f"[WARNING] {error_msg}")
                print(f"   Job: {job.get('title', 'Unknown')}")
                if self.failure_tracker:
                    self.failure_tracker.record_failure(job, "scoring", ErrorType.UNKNOWN_ERROR, error_msg)
                return None

            # Create scoring prompt
            try:
                prompt = self._create_scoring_prompt(job)
            except Exception as prompt_error:
                error_msg = f"Prompt creation failed: {type(prompt_error).__name__}: {str(prompt_error)}"
                print(f"[WARNING] {error_msg}")
                print(f"   Job: {job.get('title', 'Unknown')}")
                if self.failure_tracker:
                    self.failure_tracker.record_failure(job, "scoring", ErrorType.UNKNOWN_ERROR, error_msg)
                return None

            # Define JSON schema for the expected response
            json_schema = {
                "type": "object",
                "properties": {
                    "match_score": {
                        "type": "integer",
                        "minimum": 0,
                        "maximum": 100,
                        "description": "Match score from 0 to 100"
                    },
                    "reasoning": {
                        "type": "string",
                        "description": "Brief explanation of the score (2-3 sentences)"
                    },
                    "matched_requirements": {
                        "type": "object",
                        "description": "Dictionary of matched requirements (can be empty)"
                    }
                },
                "required": ["match_score", "reasoning", "matched_requirements"]
            }

            # Generate response with JSON schema enforcement
            try:
                response = self.client.generate_json(
                    prompt,
                    temperature=0.2,  # Lower temperature for more consistent JSON output
                    max_tokens=1024,
                    json_schema=json_schema
                )
            except requests.exceptions.Timeout:
                error_msg = "Request timed out"
                if self.failure_tracker:
                    self.failure_tracker.record_failure(job, "scoring", ErrorType.TIMEOUT_ERROR, error_msg)
                print(f"[WARNING] Timeout scoring job: {job.get('title', 'Unknown')}")
                return None
            except requests.exceptions.ConnectionError as e:
                error_msg = f"Connection error: {str(e)}"
                if self.failure_tracker:
                    self.failure_tracker.record_failure(job, "scoring", ErrorType.CONNECTION_ERROR, error_msg)
                print(f"[WARNING] Connection error scoring job: {job.get('title', 'Unknown')}")
                return None
            except json.JSONDecodeError as e:
                error_msg = f"JSON parse error: {str(e)}"
                if self.failure_tracker:
                    self.failure_tracker.record_failure(job, "scoring", ErrorType.JSON_PARSE_ERROR, error_msg)
                print(f"[WARNING] JSON parse error scoring job: {job.get('title', 'Unknown')}")
                return None

            if not response or not isinstance(response, dict):
                error_msg = f"Invalid response from AI: {type(response).__name__}"
                if self.failure_tracker:
                    self.failure_tracker.record_failure(job, "scoring", ErrorType.UNKNOWN_ERROR, error_msg)
                print(f"[WARNING] Failed to score job: {job.get('title', 'Unknown')} - Invalid response type")
                return None

            # Extract AI score and reasoning
            ai_match_score = response.get("match_score", 0)
            ai_reasoning = response.get("reasoning", "No reasoning provided")
            matched_requirements = response.get("matched_requirements", {})

            # Validate AI score
            if not isinstance(ai_match_score, (int, float)) or ai_match_score < 0 or ai_match_score > 100:
                error_msg = f"Invalid match score: {ai_match_score}"
                if self.failure_tracker:
                    self.failure_tracker.record_failure(job, "scoring", ErrorType.VALIDATION_ERROR, error_msg)
                print(f"[WARNING] Invalid match score: {ai_match_score}, defaulting to 0")
                ai_match_score = 0

            # Combine deterministic and AI scores if hybrid scoring enabled
            if use_hybrid_scoring:
                combined = self.comparison_engine.combine_scores(
                    deterministic_scores,
                    int(ai_match_score),
                    ai_reasoning
                )
                final_score = int(combined['combined_score'])
                final_reasoning = f"[Hybrid Score: {final_score}/100 = Deterministic {combined['deterministic_component']:.0f} + AI {combined['ai_component']:.0f}]\n\n{ai_reasoning}"
            else:
                final_score = int(ai_match_score)
                final_reasoning = ai_reasoning

            # Check preferences
            preference_checks = self.analyzer.validate_job_preferences(job)

            result = {
                "match_score": final_score,
                "reasoning": final_reasoning,
                "matched_requirements": matched_requirements,
                "preference_checks": preference_checks,
            }

            # Add hybrid scoring breakdown if enabled
            if use_hybrid_scoring:
                result["scoring_breakdown"] = {
                    "deterministic_score": deterministic_scores['deterministic_score'],
                    "ai_score": int(ai_match_score),
                    "combined_score": final_score,
                    "deterministic_breakdown": deterministic_scores,
                }

            return result

        except Exception as e:
            error_msg = f"Unexpected error: {str(e)}"
            if self.failure_tracker:
                self.failure_tracker.record_failure(job, "scoring", ErrorType.UNKNOWN_ERROR, error_msg)
            print(f"[WARNING] Unexpected error scoring job {job.get('title', 'Unknown')}: {e}")
            return None

    def _create_scoring_prompt(self, job: Dict[str, Any]) -> str:
        """
        Create AI prompt for job scoring with structured sections

        Args:
            job: Job dict from JobPost

        Returns:
            Formatted prompt string
        """
        # Extract structured job sections
        job_sections = extract_job_sections(job)

        # === TITLE SECTION ===
        title_text = f"""Title: {job_sections.title.job_title}
Seniority Level: {job_sections.title.seniority_level}
Job Family: {job_sections.title.job_family}"""

        # === REQUIREMENTS SECTION ===
        skills_list = "\n".join([f"  • {s}" for s in job_sections.requirements.skills]) if job_sections.requirements.skills else "  • Not specified"
        requirements_list = "\n".join([f"  • {r}" for r in job_sections.requirements.requirements]) if job_sections.requirements.requirements else "  • Not specified"

        experience_text = ""
        if job_sections.requirements.experience_years_min:
            experience_text = f"\nYears of Experience: {job_sections.requirements.experience_years_min}+"

        requirements_text = f"""Required Skills:
{skills_list}

Job Requirements:
{requirements_list}{experience_text}"""

        # === COMPENSATION SECTION ===
        salary_info = "Not specified"
        if job_sections.compensation.salary_min and job_sections.compensation.salary_max:
            salary_info = f"${job_sections.compensation.salary_min:,.0f} - ${job_sections.compensation.salary_max:,.0f} {job_sections.compensation.salary_currency} ({job_sections.compensation.salary_period})"

        benefits_list = "\n".join([f"  • {b}" for b in job_sections.compensation.benefits]) if job_sections.compensation.benefits else "  • Not specified"

        compensation_text = f"""Salary: {salary_info}
Equity: {"Yes" if job_sections.compensation.has_equity else "No"}
Bonus: {"Yes" if job_sections.compensation.has_bonus else "No"}

Benefits:
{benefits_list}"""

        # === WORK ARRANGEMENTS SECTION ===
        work_arrangements_list = "\n".join([f"  • {w}" for w in job_sections.work.work_arrangements]) if job_sections.work.work_arrangements else "  • Not specified"

        work_text = f"""Remote: {"Yes" if job_sections.work.remote else "No"}
Policy: {job_sections.work.remote_policy}
Location: {job_sections.work.location}
Job Type: {job_sections.work.job_type or "Not specified"}

Work Arrangements:
{work_arrangements_list}"""

        # === COMPANY SECTION ===
        company_description = job_sections.company.company_description or "Not available"

        # Add Glassdoor-specific fields
        company_rating_text = ""
        if job.get("company_rating"):
            company_rating_text = f"\nGlassdoor Rating: ⭐ {job['company_rating']:.1f}/5.0"

        easy_apply_text = ""
        if job.get("easy_apply"):
            easy_apply_text = "\n✓ Easy Apply Available (Quick application process)"

        occupation_text = ""
        if job.get("occupation_code"):
            confidence_pct = int((job.get("occupation_confidence", 0) * 100))
            occupation_text = f"\nJob Classification: {job['occupation_code']} ({confidence_pct}% confidence match)"

        salary_source_text = ""
        if job.get("salary_source"):
            salary_reliability = "Estimated by Glassdoor" if job["salary_source"] == "ESTIMATED" else "Provided by Employer"
            salary_source_text = f"\nSalary Data Source: {salary_reliability}"

        sponsored_text = ""
        if job.get("is_sponsored"):
            level = job.get("sponsorship_level", "Sponsored")
            sponsored_text = f"\n⚠️ This is a {level} job posting"

        company_text = f"""Company: {job_sections.company.company_name}
Size: {job_sections.company.company_size or "Not specified"} ({job_sections.company.get_size_category()})
Revenue: {job_sections.company.company_revenue or "Not specified"}
Website: {job_sections.company.company_website or "Not available"}{company_rating_text}{easy_apply_text}{occupation_text}{salary_source_text}{sponsored_text}

About the Company:
{company_description}"""

        # === DESCRIPTION ===
        description = job.get("description", "No description available")

        resume_text = self.analyzer.resume_text or "No resume provided"
        candidate_requirements_text = self.analyzer.get_requirements_text()
        preferences_text = self.analyzer.get_preferences_text()

        prompt = f"""You are an expert job matching system. Evaluate how well this job posting matches what the candidate is looking for AND whether they're qualified.

**JOB POSTING:**

**TITLE & ROLE:**
{title_text}

**REQUIREMENTS:**
{requirements_text}

**COMPENSATION & BENEFITS:**
{compensation_text}

**WORK ARRANGEMENTS:**
{work_text}

**COMPANY:**
{company_text}

**FULL JOB DESCRIPTION:**
{description}

---

**CANDIDATE RESUME:**
{resume_text}

---

**{candidate_requirements_text}**

---

**{preferences_text}**

---

**EVALUATION INSTRUCTIONS:**

**STEP 1: JOB RELEVANCE (PRIMARY FILTER)**
First, determine if this job matches what the candidate is actually searching for:
- Does the job title align with their Target Role & Career Goals?
- Is this the TYPE of role they want (e.g., Payroll Manager vs Accountant)?
- Look at the job description - is this fundamentally the right kind of position?

**IMPORTANT:** If the job title/type doesn't match their career goals, the score should be below 70 regardless of skills overlap.

**STEP 2: QUALIFICATIONS ASSESSMENT**
Next, analyze the candidate's resume to evaluate their qualifications:
- Extract their actual experience, skills, and accomplishments from the resume text
- Do they have relevant experience for this specific role?
- Do their technical skills from the resume match what the job requires?
- What's their level of experience (years, seniority)?

**STEP 3: REQUIREMENTS CHECK**
Check the MUST-HAVES and AVOID lists:
- Does the job meet all their deal-breakers (remote, salary, etc.)?
- Does the job have any red flags from their AVOID list?
- Does it align with their stated preferences?

**SCORING SCALE:**
- 90-100: Perfect match - Right job type + Strong qualifications + Meets all requirements
- 80-89: Very good match - Right job type + Good qualifications + Meets most requirements
- 70-79: Good match - Right job type + Adequate qualifications + Minor gaps
- 60-69: Tangentially related - Similar field but NOT the target role, OR right role but significant qualification gaps
- 50-59: Weak relevance - Different role type but overlapping skills
- 0-49: Wrong job type - Not what they're looking for OR violates deal-breakers

**REASONING REQUIREMENTS:**
In your 2-3 sentence explanation, address:
1. Does this job match their target role? (most important)
2. Do they have the qualifications based on their resume?
3. Any red flags or concerns?

**CRITICAL OUTPUT REQUIREMENTS:**
- You MUST respond with ONLY a JSON object
- DO NOT include any explanations, thinking, or text before or after the JSON
- DO NOT use markdown code blocks or formatting
- DO NOT add comments or notes
- The response must start with {{ and end with }}

**REQUIRED JSON FORMAT:**
{{
  "match_score": <integer from 0 to 100>,
  "reasoning": "<brief 2-3 sentence explanation addressing job type match, qualifications, and concerns>",
  "matched_requirements": {{}}
}}

Example valid response:
{{"match_score": 85, "reasoning": "This Payroll Manager role aligns perfectly with the candidate's career goals. Their resume shows 8+ years of payroll leadership experience with ADP and Workday, directly matching the job requirements. The remote position and salary range meet all their must-haves.", "matched_requirements": {{}}}}"""

        return prompt

    def score_jobs_batch(
        self, jobs: list, progress_callback: Optional[callable] = None, apply_pre_filters: bool = True
    ) -> list:
        """
        Score multiple jobs (with multi-threading support and pre-filtering)

        Args:
            jobs: List of job dicts
            progress_callback: Optional callback function(current, total, job)
            apply_pre_filters: If True, applies deterministic filters before AI scoring (default: True)

        Returns:
            List of jobs with added match_score, reasoning, etc.
        """
        # Reset tracking lists
        self.failed_jobs = []
        self.rejected_jobs = []
        self.filtered_jobs = []

        # Get thread count from environment
        max_workers = int(os.getenv("MATCH_THREADS", "4"))
        total = len(jobs)

        # Get list of already-processed jobs from checkpoint
        processed_urls = set()
        if self.checkpoint_manager:
            processed_urls = set(self.checkpoint_manager.get_processed_urls("scoring"))

        # Filter out already-processed jobs
        jobs_to_process = [job for job in jobs if job.get("job_url", "") not in processed_urls]

        if not jobs_to_process:
            return []

        # APPLY DETERMINISTIC PRE-FILTERS
        if apply_pre_filters:
            print(f"\n🔍 Applying deterministic filters to {len(jobs_to_process)} jobs...")

            passed_jobs, rejected_jobs, filter_stats = apply_filters_to_jobs(
                jobs_to_process,
                self.analyzer.candidate_profile,
                self.analyzer.preferences
            )

            # Track filtered jobs
            self.filtered_jobs = rejected_jobs

            # Print filter statistics
            print(f"✅ Filters passed: {filter_stats['passed_jobs']} jobs")
            print(f"❌ Filters rejected: {filter_stats['rejected_jobs']} jobs")
            print(f"📊 Pass rate: {filter_stats['pass_rate']*100:.1f}%")

            if filter_stats['rejection_reasons']:
                print(f"\n📋 Rejection breakdown:")
                for reason, count in filter_stats['rejection_reasons'].items():
                    print(f"   • {reason}: {count} jobs")

            # Use filtered jobs for AI scoring
            jobs_to_process = passed_jobs
        else:
            # TITLE RELEVANCE FILTER (legacy fallback if pre-filters disabled)
            relevant_jobs = []
            for job in jobs_to_process:
                if self._is_title_relevant(job):
                    relevant_jobs.append(job)
                else:
                    self.rejected_jobs.append(job)

            jobs_to_process = relevant_jobs

        if not jobs_to_process:
            print("\n[WARNING] No jobs passed filters!")
            return []

        # Update total to reflect filtered count
        total = len(jobs_to_process)

        print(f"\n[INFO] Scoring {len(jobs_to_process)} jobs with AI...\n")

        # Thread-safe progress tracking and failure tracking
        progress_lock = threading.Lock()
        completed_count = [0]  # Use list for mutable reference

        def process_single_job(job):
            """Process a single job and update progress"""
            job_url = job.get("job_url", "")

            try:
                # Score the job
                score_result = self.score_job(job)

                if score_result:
                    # Add scoring results to job
                    job_with_score = {**job, **score_result}
                else:
                    # Job failed to score, add with 0 score
                    job_with_score = {
                        **job,
                        "match_score": 0,
                        "reasoning": "Failed to score job",
                        "matched_requirements": {},
                        "preference_checks": {},
                    }
                    # Track failure (thread-safe)
                    with progress_lock:
                        self.failed_jobs.append(job)

                # Save checkpoint after each job (thread-safe)
                if self.checkpoint_manager and job_url:
                    self.checkpoint_manager.mark_job_completed("scoring", job_url)

                # Update progress (thread-safe)
                with progress_lock:
                    completed_count[0] += 1
                    if progress_callback:
                        progress_callback(completed_count[0], total, job)

                return job_with_score

            except Exception as e:
                print(f"[WARNING] Error processing job {job.get('title', 'Unknown')}: {e}")

                # Record failure
                error_msg = f"Thread execution error: {str(e)}"
                if self.failure_tracker:
                    self.failure_tracker.record_failure(job, "scoring", ErrorType.UNKNOWN_ERROR, error_msg)

                # Track failure
                with progress_lock:
                    self.failed_jobs.append(job)

                # Return job with error state
                return {
                    **job,
                    "match_score": 0,
                    "reasoning": f"Error during processing: {str(e)}",
                    "matched_requirements": {},
                    "preference_checks": {},
                }

        # Process jobs using smooth batch processor (maintains constant GPU load)
        processor = SmoothBatchProcessor(max_workers=max_workers)
        scored_jobs = processor.process_batch(jobs_to_process, process_single_job)

        return scored_jobs

    def score_jobs_batch_queued(
        self, jobs: list, progress_callback: Optional[callable] = None, apply_pre_filters: bool = True
    ) -> list:
        """
        Score multiple jobs using batch queue processing (constant GPU load)

        This method pre-generates all prompts and queues AI requests continuously
        to maintain constant GPU utilization, eliminating power spikes.

        Args:
            jobs: List of job dicts
            progress_callback: Optional callback function(current, total, job)
            apply_pre_filters: If True, applies deterministic filters before AI scoring (default: True)

        Returns:
            List of jobs with added match_score, reasoning, etc.
        """
        import time
        from .batch_queue_processor import BatchQueueProcessor

        # Reset tracking lists
        self.failed_jobs = []
        self.rejected_jobs = []
        self.filtered_jobs = []

        # Get configuration
        max_workers = int(os.getenv("MATCH_THREADS", "4"))
        queue_delay_ms = int(os.getenv("BATCH_QUEUE_DELAY_MS", "50"))

        # Filter out already-processed jobs from checkpoint
        jobs_to_process = jobs
        if self.checkpoint_manager:
            processed_urls = set(self.checkpoint_manager.get_processed_urls("scoring"))
            jobs_to_process = [job for job in jobs if job.get("job_url", "") not in processed_urls]

        if not jobs_to_process:
            return []

        # APPLY DETERMINISTIC PRE-FILTERS
        if apply_pre_filters:
            print(f"\n🔍 Applying deterministic filters to {len(jobs_to_process)} jobs...")

            passed_jobs, rejected_jobs, filter_stats = apply_filters_to_jobs(
                jobs_to_process,
                self.analyzer.candidate_profile,
                self.analyzer.preferences
            )

            # Track filtered jobs
            self.filtered_jobs = rejected_jobs

            # Print filter statistics
            print(f"✅ Filters passed: {filter_stats['passed_jobs']} jobs")
            print(f"❌ Filters rejected: {filter_stats['rejected_jobs']} jobs")
            print(f"📊 Pass rate: {filter_stats['pass_rate']*100:.1f}%")

            if filter_stats['rejection_reasons']:
                print(f"\n📋 Rejection breakdown:")
                for reason, count in filter_stats['rejection_reasons'].items():
                    print(f"   • {reason}: {count} jobs")

            # Use filtered jobs for AI scoring
            jobs_to_process = passed_jobs
        else:
            # TITLE RELEVANCE FILTER (legacy fallback if pre-filters disabled)
            relevant_jobs = []
            for job in jobs_to_process:
                if self._is_title_relevant(job):
                    relevant_jobs.append(job)
                else:
                    self.rejected_jobs.append(job)

            jobs_to_process = relevant_jobs

        if not jobs_to_process:
            print("\n[WARNING] No jobs passed filters!")
            return []

        print(f"\n[INFO] Scoring {len(jobs_to_process)} jobs with AI (batch queue mode)...\n")

        # PHASE 0: PRE-COMPUTE ALL DETERMINISTIC SCORES (CPU phase before GPU)
        print(f"[INFO] Computing deterministic scores for {len(jobs_to_process)} jobs...")
        phase_start = time.time()

        deterministic_scores_map = {}
        for job in jobs_to_process:
            job_url = job.get("job_url", "")
            try:
                deterministic_scores_map[job_url] = self.comparison_engine.calculate_deterministic_score(job)
            except Exception as e:
                print(f"[WARNING] Deterministic scoring failed for {job.get('title', 'Unknown')}: {e}")
                deterministic_scores_map[job_url] = None

        print(f"[INFO] Deterministic scoring: {time.time() - phase_start:.2f}s")

        # Define prompt generator (now uses pre-computed scores)
        def prompt_generator(job: Dict[str, Any]) -> str:
            """Generate scoring prompt for a job"""
            # Use pre-computed deterministic score (NO calculation here!)
            job_url = job.get("job_url", "")
            job['_deterministic_scores'] = deterministic_scores_map.get(job_url)
            return self._create_scoring_prompt(job)

        # Define AI parameters for async batch mode
        json_schema = {
            "type": "object",
            "properties": {
                "match_score": {
                    "type": "integer",
                    "minimum": 0,
                    "maximum": 100,
                    "description": "Match score from 0 to 100"
                },
                "reasoning": {
                    "type": "string",
                    "description": "Brief explanation of the score (2-3 sentences)"
                },
                "matched_requirements": {
                    "type": "object",
                    "description": "Dictionary of matched requirements (can be empty)"
                }
            },
            "required": ["match_score", "reasoning", "matched_requirements"]
        }

        # Define AI executor (fallback for thread-based mode)
        def ai_executor(prompt: str) -> Optional[Dict[str, Any]]:
            """Execute AI request for scoring (fallback if async batch not available)"""
            try:
                response = self.client.generate_json(
                    prompt,
                    temperature=0.2,
                    max_tokens=1024,
                    json_schema=json_schema
                )
                return response
            except Exception as e:
                return None

        # Define result merger
        def result_merger(job: Dict[str, Any], result: Optional[Dict[str, Any]]) -> Dict[str, Any]:
            """Merge AI result into job dict"""
            # Get deterministic scores from temporary storage
            deterministic_scores = job.pop('_deterministic_scores', None)

            if result and isinstance(result, dict):
                # Extract AI score and reasoning
                ai_match_score = result.get("match_score", 0)
                ai_reasoning = result.get("reasoning", "No reasoning provided")
                matched_requirements = result.get("matched_requirements", {})

                # Validate AI score
                if not isinstance(ai_match_score, (int, float)) or ai_match_score < 0 or ai_match_score > 100:
                    ai_match_score = 0

                # Combine with deterministic scores if available
                if deterministic_scores:
                    combined = self.comparison_engine.combine_scores(
                        deterministic_scores,
                        int(ai_match_score),
                        ai_reasoning
                    )
                    final_score = int(combined['combined_score'])
                    final_reasoning = f"[Hybrid Score: {final_score}/100 = Deterministic {combined['deterministic_component']:.0f} + AI {combined['ai_component']:.0f}]\n\n{ai_reasoning}"

                    scoring_breakdown = {
                        "deterministic_score": deterministic_scores['deterministic_score'],
                        "ai_score": int(ai_match_score),
                        "combined_score": final_score,
                        "deterministic_breakdown": deterministic_scores,
                    }
                else:
                    final_score = int(ai_match_score)
                    final_reasoning = ai_reasoning
                    scoring_breakdown = None

                # Check preferences
                preference_checks = self.analyzer.validate_job_preferences(job)

                # Build result
                job_with_score = {
                    **job,
                    "match_score": final_score,
                    "reasoning": final_reasoning,
                    "matched_requirements": matched_requirements,
                    "preference_checks": preference_checks,
                }

                if scoring_breakdown:
                    job_with_score["scoring_breakdown"] = scoring_breakdown

                return job_with_score
            else:
                # Failure case
                self.failed_jobs.append(job)
                return {
                    **job,
                    "match_score": 0,
                    "reasoning": "Failed to score job",
                    "matched_requirements": {},
                    "preference_checks": {},
                }

        # Create batch queue processor
        processor = BatchQueueProcessor(
            max_workers=max_workers,
            queue_delay_ms=queue_delay_ms
        )

        # Process batch with async batch mode enabled
        scored_jobs = processor.process_batch(
            jobs=jobs_to_process,
            prompt_generator=prompt_generator,
            ai_executor=ai_executor,
            result_merger=result_merger,
            progress_callback=progress_callback,
            checkpoint_manager=self.checkpoint_manager,
            checkpoint_stage="scoring",
            failure_tracker=self.failure_tracker,
            failure_stage="scoring",
            llama_client=self.client,  # Pass LlamaClient for async batch mode
            temperature=0.2,  # AI generation temperature
            max_tokens=1024,  # Max tokens for AI generation
            json_schema=json_schema,  # JSON schema for validation
        )

        return scored_jobs

    def get_failed_jobs(self) -> List[Dict[str, Any]]:
        """
        Get list of jobs that failed during the last batch scoring

        Returns:
            List of failed job dicts
        """
        return self.failed_jobs

    def get_rejected_jobs(self) -> List[Dict[str, Any]]:
        """
        Get list of jobs rejected by title filter

        Returns:
            List of rejected job dicts
        """
        return self.rejected_jobs

    def get_filtered_jobs(self) -> List[Dict[str, Any]]:
        """
        Get list of jobs rejected by deterministic filters

        Returns:
            List of filtered job dicts with rejection reasons
        """
        return self.filtered_jobs

    def filter_by_score(
        self, jobs: list, min_score: int = 70
    ) -> tuple[list, list]:
        """
        Filter jobs by minimum match score

        Args:
            jobs: List of scored jobs
            min_score: Minimum score threshold

        Returns:
            Tuple of (matched_jobs, rejected_jobs)
        """
        matched = [job for job in jobs if job.get("match_score", 0) >= min_score]
        rejected = [job for job in jobs if job.get("match_score", 0) < min_score]

        return matched, rejected


if __name__ == "__main__":
    # Test the scorer
    print("Testing MatchScorer...")

    from dotenv import load_dotenv

    load_dotenv()

    # Initialize components
    client = LlamaClient()
    analyzer = ResumeAnalyzer()

    print("Loading resume and requirements...")
    try:
        analyzer.load_all()
        print("Loaded successfully")
    except Exception as e:
        print(f"X Error loading: {e}")
        exit(1)

    # Create scorer
    scorer = MatchScorer(client, analyzer)

    # Test with a sample job
    sample_job = {
        "title": "Senior Payroll Manager",
        "company": "Test Company",
        "location": "Remote",
        "job_type": "full-time",
        "remote": True,
        "salary_min": 80000,
        "salary_max": 120000,
        "salary_currency": "USD",
        "salary_period": "yearly",
        "description": """
We are seeking an experienced Senior Payroll Manager to oversee our multi-state payroll operations.

Requirements:
- 5+ years payroll management experience
- Strong knowledge of ADP and Workday
- Multi-state payroll compliance expertise
- Bachelor's degree in Accounting or Finance
- Advanced Excel skills
- Strong attention to detail
        """,
    }

    print("\nScoring sample job...")
    result = scorer.score_job(sample_job)

    if result:
        print(f"\nMatch Score: {result['match_score']}/100")
        print(f"Reasoning: {result['reasoning']}")
        print(f"\nMatched Requirements:")
        for req, matched in result["matched_requirements"].items():
            status = "✅" if matched else "X"
            print(f"  {status} {req}")
    else:
        print("X Scoring failed")

    print("\nMatchScorer test complete")
