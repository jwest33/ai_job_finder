# ============================================================================
# Docker Environment Configuration Template
# ============================================================================
# Copy this file to .env and fill in your actual values
#
# Usage:
#   cp .env.docker.example .env
#   # Edit .env with your values
#   docker-compose up -d

# ============================================================================
# MCP Server Configuration
# ============================================================================

# Enable/disable MCP server
MCP_SERVER_ENABLED=true

# Server host (the hostname the MCP server binds to)
# - Use 0.0.0.0 to listen on all interfaces (recommended for Docker)
# - Use localhost to only listen on loopback
MCP_SERVER_HOST=localhost

# Server port
MCP_SERVER_PORT=3000

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
MCP_LOG_LEVEL=INFO

# Authentication
MCP_AUTH_ENABLED=false
# Generate with: python cli.py mcp generate-token
MCP_AUTH_TOKEN=your-generated-token-here

# Feature flags
MCP_ALLOW_DESTRUCTIVE_OPERATIONS=true
MCP_REQUIRE_CONFIRMATION=true
MCP_ENABLE_AUDIT_LOG=true

# ============================================================================
# MCP Web Client Configuration
# ============================================================================

# Enable/disable web client
MCP_WEB_CLIENT_ENABLED=true

# Web client port
MCP_WEB_CLIENT_PORT=5000

# Flask secret key (generate a secure random string!)
# Generate with: python -c "import secrets; print(secrets.token_hex(32))"
FLASK_SECRET_KEY=your-secret-key-here

# MCP server connection
MCP_SERVER_URL=http://localhost:3000

# ============================================================================
# llama-server Configuration
# ============================================================================

# llama-server URL
# Use host.docker.internal to access host machine from Docker
LLAMA_SERVER_URL=http://host.docker.internal:8080

# Model configuration
LLAMA_MODEL_PATH=/path/to/your/model.gguf
LLAMA_CONTEXT_SIZE=8192
LLAMA_TEMPERATURE=0.3
LLAMA_MAX_TOKENS=2560

# ============================================================================
# Background Job Scheduling (Cron)
# ============================================================================

# Enable/disable scheduled jobs
ENABLE_SCRAPER_CRON=true
ENABLE_MATCHER_CRON=true

# Cron schedules (crontab format)
# Format: minute hour day month weekday
# Examples:
#   0 9 * * *     = Every day at 9:00 AM
#   30 9 * * *    = Every day at 9:30 AM
#   0 */6 * * *   = Every 6 hours
#   0 9 * * 1-5   = Weekdays at 9:00 AM
CRON_SCHEDULE_SCRAPER=0 9 * * *
CRON_SCHEDULE_MATCHER=30 9 * * *

# ============================================================================
# Job Scraper Configuration
# ============================================================================

# Proxy configuration
USE_PROXY=true

# IPRoyal proxy credentials (only needed if USE_PROXY=true)
IPROYAL_HOST=geo.iproyal.com
IPROYAL_PORT=12321
IPROYAL_USERNAME=your-iproyal-username
IPROYAL_PASSWORD=your-iproyal-password

PROXY_URL=iproyal.com:

# =============================================================================
# Job Search Configuration
# =============================================================================

# Number of results to fetch per job/location combination
# Note: Each 100 results = ~1 API request
RESULTS_PER_SEARCH=500

# Output format: csv, json, or both
OUTPUT_FORMAT=both

# Remove duplicate job postings based on URL
DEDUPLICATE=true

# IP Rotation for comprehensive job discovery
# Run each search multiple times with different proxy IPs to capture location-based variations
# Example: With 8 jobs × 1 location × 3 IPs = 24 total searches
# Bandwidth: ~1.2 MB for 3 IPs, ~2.0 MB for 5 IPs
PROXY_ROTATION_COUNT=2

# =============================================================================
# Rate Limiting
# =============================================================================

# Delay between requests in seconds (default: 2.5)
# Increase if you're getting rate limited
RATE_LIMIT_DELAY=2.5

# =============================================================================
# Testing Configuration (Optional)
# =============================================================================

# Set to true for test mode with additional logging
TEST_MODE=false

# Maximum requests per session (safety limit, 0 = unlimited)
MAX_REQUESTS_PER_SESSION=0

# =============================================================================
# AI Job Matcher Configuration
# =============================================================================

# llama-server URL (comma-separated list of URLs to try in order)
LLAMA_SERVER_URL=http://localhost:8080,http://host.docker.internal:8080


# Path to the GGUF model file
LLAMA_MODEL_PATH=C:\models\Qwen3-30B-A3B-Instruct-2507\Qwen3-30B-A3B-Instruct-2507-Q6_K.gguf

# Model parameters
LLAMA_CONTEXT_SIZE=8192
LLAMA_TEMPERATURE=0.3
LLAMA_MAX_TOKENS=2560
LLAMA_REQUEST_TIMEOUT=300

# Job Matching Configuration
# Note: RESUME_PATH, REQUIREMENTS_PATH, REPORT_OUTPUT_DIR are now managed by ProfilePaths
# They automatically resolve to: profiles/{ACTIVE_PROFILE}/templates/ and profiles/{ACTIVE_PROFILE}/reports/
MIN_MATCH_SCORE=60

# Multi-threading Configuration
# Number of parallel threads for job matching (scoring, analysis, optimization)
# Recommended: 4-8 threads for optimal performance
# Higher values = faster processing but more CPU/memory usage
# Set to 1 to disable multi-threading
MATCH_THREADS=8
ASYNC_BATCH_SIZE=true
ASYNC_BATCH_SIZE=0

# Batch Queue Configuration (for constant GPU load)
BATCH_QUEUE_MODE=true
# Delay between queuing requests (milliseconds)
# 0 = submit as fast as possible (best for GPU saturation)
# 50-100 = smooth request flow (use if llama-server gets overwhelmed)
BATCH_QUEUE_DELAY_MS=0

# Note: JOB_TRACKER_DB and FAILURE_TRACKER_DB are now managed by ProfilePaths
# They automatically resolve to: profiles/{ACTIVE_PROFILE}/data/job_tracker.db and job_failures.db

# =============================================================================
# Deterministic Filters Configuration
# =============================================================================
# Enable/disable individual pre-filters for faster job screening
# Pre-filters run BEFORE AI scoring to eliminate irrelevant jobs

# Deterministic Filters (set to true/false)
FILTER_TITLE_ENABLED=true
FILTER_SALARY_ENABLED=false
FILTER_LOCATION_ENABLED=false
FILTER_REMOTE_ENABLED=true
FILTER_JOB_TYPE_ENABLED=true
FILTER_COMPANY_SIZE_ENABLED=false
FILTER_POSTING_AGE_ENABLED=true

# =============================================================================
# Retry Configuration (for failed jobs)
# =============================================================================

# Override temperature for retry attempts (higher = more creative responses)
# Default: uses LLAMA_TEMPERATURE if not set
# Recommended: 0.5-0.7 for retries (higher than normal processing)
# RETRY_TEMPERATURE=0.5

# Override max_tokens for retry attempts (more tokens = longer responses)
# Default: uses LLAMA_MAX_TOKENS if not set
# Recommended: 4096 for complex jobs that failed with validation errors
# RETRY_MAX_TOKENS=4096

# Maximum retry attempts per job before marking as permanently failed
# RETRY_MAX_ATTEMPTS=3

# Delay between retry attempts in seconds
# RETRY_DELAY_SECONDS=5

# =============================================================================
# Email Configuration
# =============================================================================

# Enable/disable automatic email delivery of reports
EMAIL_ENABLED=true

# Email addresses to send reports to (comma-separated, configure with: python setup_email.py)
EMAIL_RECIPIENT=@gmail.com

# Send email automatically when pipeline completes (requires EMAIL_ENABLED=true)
EMAIL_SEND_ON_COMPLETION=true

# Only send email if at least this many matches are found
EMAIL_MIN_MATCHES=1

# Email subject line prefix
EMAIL_SUBJECT_PREFIX='Job Report'
ACTIVE_PROFILE='default'

# =============================================================================
# MCP Server Configuration
# =============================================================================

# MCP Server host (use localhost for same-container, or 0.0.0.0 for external access)
MCP_SERVER_HOST=localhost

# MCP Server port
MCP_SERVER_PORT=3000

# Enable/disable MCP server
MCP_SERVER_ENABLED=true

# MCP Authentication
MCP_AUTH_ENABLED=false

# MCP Web Client
MCP_WEB_CLIENT_ENABLED=true
MCP_WEB_CLIENT_PORT=5000

# Flask secret key for web sessions
FLASK_SECRET_KEY=''
MCP_AUTH_TOKEN=''
MCP_SERVER_PATH='mcp_server'
