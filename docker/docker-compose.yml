# =============================================================================
# Docker Compose Configuration for Job Search Application
# =============================================================================
# This file defines the complete stack for running the job search system
# with persistent volumes and configurable scheduling.

services:
  job-search:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: job-search:latest
    container_name: job-search-app
    hostname: job-search
    restart: unless-stopped

    # Load environment variables from .env file in project root
    env_file:
      - ../.env

    # Environment variables (override .env values if needed)
    environment:
      # Cron scheduling
      - CRON_SCHEDULE_SCRAPER=${CRON_SCHEDULE_SCRAPER:-0 9 * * *}
      - CRON_SCHEDULE_MATCHER=${CRON_SCHEDULE_MATCHER:-30 9 * * *}
      - ENABLE_SCRAPER_CRON=${ENABLE_SCRAPER_CRON:-true}
      - ENABLE_MATCHER_CRON=${ENABLE_MATCHER_CRON:-true}

      # MCP Server Configuration
      - MCP_SERVER_ENABLED=${MCP_SERVER_ENABLED:-true}
      - MCP_SERVER_HOST=0.0.0.0
      - MCP_SERVER_PORT=${MCP_SERVER_PORT:-3000}
      - MCP_LOG_LEVEL=${MCP_LOG_LEVEL:-INFO}
      - MCP_AUTH_ENABLED=${MCP_AUTH_ENABLED:-false}
      - MCP_AUTH_TOKEN=${MCP_AUTH_TOKEN}

      # MCP Web Client Configuration
      - MCP_WEB_CLIENT_ENABLED=${MCP_WEB_CLIENT_ENABLED:-true}
      - MCP_WEB_CLIENT_HOST=0.0.0.0
      - MCP_WEB_CLIENT_PORT=${MCP_WEB_CLIENT_PORT:-5000}
      - FLASK_SECRET_KEY=${FLASK_SECRET_KEY:-dev-secret-key-change-in-production}

      # MCP Integration URLs
      - MCP_SERVER_URL=http://localhost:${MCP_SERVER_PORT:-3000}

      # llama-server connection (use host.docker.internal for local server)
      - LLAMA_SERVER_URL=${LLAMA_SERVER_URL:-http://host.docker.internal:8080}

      # All other env vars are loaded from .env file automatically
      - TZ=${TZ:-America/New_York}

    # Volume mounts
    volumes:
      # Profile-based directory structure
      - ../profiles:/app/profiles:rw

      # MCP Web Client conversations
      - ../conversations:/app/conversations:rw

      # Global configuration
      - ../.env:/app/.env:rw
      - ../credentials:/app/credentials:rw
      - ../assets:/app/assets:ro

      # Logs
      - job-logs:/var/log/job-search

      # Legacy mounts (for backward compatibility - can be removed after migration)
      # - ./templates:/app/templates:rw
      # - ./data:/app/data:rw
      # - ./reports:/app/reports:rw

    # Network configuration
    # Note: network_mode: host only works on Linux. On Windows/Mac, use bridge with port mappings.
    # network_mode: host

    # Port mappings for bridge network (Windows/Mac compatible)
    # Bind to 0.0.0.0 to allow access from network (not just localhost)
    ports:
      - "0.0.0.0:3000:3000"  # MCP Server
      - "0.0.0.0:5000:5000"  # MCP Web Client

    # Resource limits (adjust based on your system)
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

    # Health check
    healthcheck:
      test: ["CMD-SHELL", "(test -f /var/run/crond.pid || exit 0) && (test \"$MCP_SERVER_ENABLED\" != \"true\" || curl -f http://localhost:${MCP_SERVER_PORT:-3000}/health || exit 1) && (test \"$MCP_WEB_CLIENT_ENABLED\" != \"true\" || curl -f http://localhost:${MCP_WEB_CLIENT_PORT:-5000}/api/health || exit 1)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Optional: Run llama-server in a separate container
  # Uncomment this section if you want llama-server in Docker too
  # llama-server:
  #   image: ghcr.io/ggerganov/llama.cpp:server
  #   container_name: llama-server
  #   hostname: llama-server
  #   restart: unless-stopped
  #   command:
  #     - --model
  #     - /models/your-model.gguf
  #     - --ctx-size
  #     - "8192"
  #     - --host
  #     - "0.0.0.0"
  #     - --port
  #     - "8080"
  #   volumes:
  #     - ${LLAMA_MODEL_PATH}:/models:ro
  #   networks:
  #     - job-search-network
  #   ports:
  #     - "8080:8080"
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '4.0'
  #         memory: 8G

# Named volumes for persistent data
volumes:
  job-logs:
    driver: local

# Network configuration (if not using host network)
# networks:
#   job-search-network:
#     driver: bridge
